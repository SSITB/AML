{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2017 Edition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection in Network Traffic with K-means clustering\n",
    "\n",
    "We can categorize machine learning algorithms into two main groups: **supervised learning** and **unsupervised learning**. With supervised learning algorithms, in order to predict unknown values for new data, we have to know the target value for many previously-seen examples. In contrast, unsupervised learning algorithms explore the data which has no target attribute to find some intrinsic structures in them.\n",
    "\n",
    "Clustering is a technique for finding similar groups in data, called **clusters**. Clustering is often called an unsupervised learning task as no class values denoting an a priori grouping of the data instances are given.\n",
    "\n",
    "In this notebook, we will use K-means, a very well-known clustering algorithm to detect anomaly network connections based on statistics about each of them. A thorough overview of K-means clustering, from a research perspective, can be found in the following wonderful [tutorial](http://theory.stanford.edu/~sergei/slides/kdd10-thclust.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "We expect students to:\n",
    "* Learn (or revise) and understand the K-means algorithm\n",
    "* Implement a simple K-means algorithm\n",
    "* Use K-means to detect anomalies network connection data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "1. In section 1, we will have an overview about K-means then implement a simple version of it.\n",
    "2. In section 2, we build models with and without categorical features.\n",
    "3. Finally, in the last section, using our models, we will detect unusual connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. K-means\n",
    "## 1.1. Introduction\n",
    "Clustering is a typical and well-known type of unsupervised learning. Clustering algorithms try to find natural groupings in data. Similar data points (according to some notion of similarity) are considered in the same group. We call these groups **clusters**.\n",
    "\n",
    "K-Means clustering is a simple and widely-used clustering algorithm. Given value of $k$, it tries to build $k$ clusters from samples in the dataset. Therefore, $k$ is an hyperparameter of the model. The right value of $k$ is not easy to determine, as it highly depends on the data set and the way that data is featurized.\n",
    "\n",
    "To measure the similarity between any two data points, K-means requires the definition of a distance function between data points. What is a distance? It is a value that indicates how close two data points are in their space. In particular, when data points lie in a $d$-dimensional space, the Euclidean distance is a good choice of a distance function, and is supported by MLLIB.\n",
    "\n",
    "In K-means, a cluster is a group of points, with a representative entity called a centroid. A centroid is also a point in the data space: the center of all the points that make up the cluster. It's defined to be the arithmetic mean of the points. In general, when working with K-means, each data sample is represented in a $d$-dimensional numeric vector, for which it is easier to define an appropriate distance function. As a consequence, in some applications, the original data must be transformed into a different representation, to fit the requirements of K-means.\n",
    "\n",
    "## 1.2. How does it work?\n",
    "Given $k$, the K-means algorithm works as follows:\n",
    "\n",
    "1. Randomly choose $k$ data points (seeds) to be the initial centroids\n",
    "2. Assign each data point to the **closest centroid**\n",
    "3. Re-compute (update) the centroids using the current cluster memberships\n",
    "4. If a convergence criterion is not met, go to step 2\n",
    "\n",
    "We can also terminate the algorithm when it reaches an iteration budget, which yields an approximate result.\n",
    "From the pseudo-code of the algorithm, we can see that K-means clustering results can be sensitive to the order in which data samples in the data set are explored. A sensible practice would be to run the analysis several times, randomizing objects order; then, average the cluster centers of those runs and input the centers as initial ones for one final run of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Illustrative example\n",
    "One of the best ways to study an algorithm is trying implement it.\n",
    "In this section, we will go step by step to implement a simple K-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "#### Question 1.1\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Complete the below function to calculate an Euclidean distance between any two points in $d$-dimensional data space\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# calculate distance between two d-dimensional points\n",
    "def euclidean_distance(p1, p2):\n",
    "    return np.sqrt(np.sum([(d1-d2)**2 for d1, d2 in zip(p1, p2)]))\n",
    "\n",
    "# test our function\n",
    "assert (round(euclidean_distance([1,2,3] , [10,18,12]), 2) == 20.45), \"Function's wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.2\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Given a data point and the current set of centroids, complete the function below to find the index of the closest centroid for that data point.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_closest_centroid(datapoint, centroids):\n",
    "    # find the index of the closest centroid of the given data point.\n",
    "    return min(enumerate(centroids), key=lambda x: euclidean_distance(datapoint, x[1]))[0]\n",
    "\n",
    "assert(find_closest_centroid( [1,1,1], [ [2,1,2], [1,2,1], [3,1,2] ] ) == 1), \"Function's wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.3\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Write a function to randomize `k` initial centroids.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(22324)\n",
    "\n",
    "# randomize initial centroids\n",
    "def randomize_centroids(data, k):\n",
    "    centroids = data\n",
    "    np.random.shuffle(centroids)\n",
    "    return centroids[:k]\n",
    "\n",
    "assert(len(\n",
    "    randomize_centroids(np.array([np.array([2,1,2]), np.array([1,2,1]), np.array([3,1,2])]), \n",
    "        2)) == 2), \"Wrong function\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.4\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Write function `check_converge` to check the stop criteria of the algorithm.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 5\n",
    "\n",
    "# return True if clusters have converged , otherwise, return False  \n",
    "def check_converge(centroids, old_centroids, num_iterations, threshold=0):\n",
    "    # if it reaches an iteration budget\n",
    "    if num_iterations > MAX_ITERATIONS:\n",
    "        return True\n",
    "    # check if the centroids don't move (or very slightly)\n",
    "    dist = [euclidean_distance(curr, old) for curr, old in zip(centroids, old_centroids)]\n",
    "    if all(dist[x]<threshold for x in range(len(dist))):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.5\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Write function `update_centroid` to update the new positions for the current centroids based on the position of their members.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# centroids: a list of centers\n",
    "# cluster: a list of k elements. Each element i-th is a list of data points that are assigned to center i-th\n",
    "def update_centroids(centroids, cluster):\n",
    "    for i, c in enumerate(cluster):\n",
    "        centroids[i] = sum(c)/len(c)\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.6\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Complete the K-means algorithm skeleton below, with the functions you wrote above.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data : set of data points\n",
    "# k : number of clusters\n",
    "# centroids: initial list of centroids\n",
    "def kmeans(data, k=2, centroids=None):\n",
    "    \n",
    "    # randomize the centroids if they are not given\n",
    "    if not centroids:\n",
    "        centroids = randomize_centroids(data, k)\n",
    "\n",
    "    old_centroids = centroids[:]\n",
    "\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        iterations += 1\n",
    "\n",
    "        # init empty clusters\n",
    "        clusters = [[] for i in range(k)]\n",
    "\n",
    "        # assign each data point to the closest centroid\n",
    "        for point in data:\n",
    "            # find the closest center of each data point\n",
    "            centroid_idx = find_closest_centroid(point, centroids)\n",
    "            \n",
    "            # assign datapoint to the closest cluster\n",
    "            clusters[centroid_idx].append(point)\n",
    "        \n",
    "        # keep the current position of centroids before changing them\n",
    "        old_centroids = centroids[:]\n",
    "        \n",
    "        # update centroids\n",
    "        centroids = update_centroids(clusters, clusters)\n",
    "        \n",
    "        # if the stop criteria are met, stop the algorithm\n",
    "        if check_converge(centroids, old_centroids, iterations):\n",
    "            break\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will test our algorithm on [Fisher's Iris dataset](http://en.wikipedia.org/wiki/Iris_flower_data_set), and plot the resulting clusters in 3D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.7\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "The code below can be used to test your algorithm with three different datasets: `Iris`, `Moon` and `Blob`.\n",
    "Run your algorithm to cluster datapoints in these datasets, plot the results and discuss about them. Do you think that our algorithm works well? Why?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the sourcecode in this cell is inspired from \n",
    "# https://gist.github.com/bbarrilleaux/9841297\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets, cluster\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# load data\n",
    "iris = datasets.load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "# do the clustering\n",
    "centers = kmeans(X_iris, k=3)\n",
    "labels = [find_closest_centroid(p, centers) for p in X_iris]\n",
    "\n",
    "#plot the clusters in color\n",
    "fig = plt.figure(1, figsize=(8, 8))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, 1, 1], elev=8, azim=200)\n",
    "plt.cla()\n",
    "ax.scatter(X_iris[:, 3], X_iris[:, 0], X_iris[:, 2], c=labels)\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('Petal width')\n",
    "ax.set_ylabel('Sepal length')\n",
    "ax.set_zlabel('Petal length')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Here we use sci-kit learn implementation of K-means\n",
    "# centers =cluster.KMeans(n_clusters=3)\n",
    "# centers.fit(X_iris) \n",
    "# labels = centers2.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# moon\n",
    "np.random.seed(0)\n",
    "X, y = datasets.make_moons(2000, noise=0.1)\n",
    "\n",
    "centers = kmeans(X, k=2)\n",
    "labels = [find_closest_centroid(p, centers) for p in X]\n",
    "\n",
    "fig = plt.figure(1, figsize=(8, 8))\n",
    "plt.clf()\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=labels, cmap=plt.cm.Spectral)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "With this dataset, we can clearly see that the clustering technique is not working properly. The visual division of the two clusters is totally different than the effective division computed by the algorithm.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# blob\n",
    "np.random.seed(0)\n",
    "X, y = datasets.make_blobs(n_samples=2000, centers=3, n_features=20, random_state=0)\n",
    "\n",
    "centers = kmeans(X, k=3)\n",
    "labels = [find_closest_centroid(p, centers) for p in X]\n",
    "\n",
    "plt.figure(1, figsize=(20, 8))\n",
    "plt.subplot(\"121\")\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=labels, cmap=plt.cm.Spectral)\n",
    "\n",
    "# blob with randomized initial centers\n",
    "centers = kmeans(X, k=3)\n",
    "labels = [find_closest_centroid(p, centers) for p in X]\n",
    "\n",
    "plt.subplot(\"122\")\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=labels, cmap=plt.cm.Spectral)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "The blob dataset above shows another problem with clustering. The left plot represents the data clusterized in 3 clusters with the centroids initialized with a random seed set to 0. The right plot instead has no constraints on the initial centroids and so they are always chosen randomly.\n",
    "<br><br>\n",
    "This example shows us that the effectivness and the accuracy of the clustering technique is highly dependant on different factors and one of them is the choice of the initial centroids.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's enough about K-means for now. In the next section, we will apply MMLIB's K-means on Spark to deal with a large data in the real usecase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Usecase: Network Intrusion \n",
    "\n",
    "Some attacks attempt to flood a computer with network traffic. In some other cases, attacks attempt to exploit flaws in networking software in order to gain unauthorized access to a computer. Detecting an exploit in an incredibly large haystack of network requests is not easy.\n",
    "\n",
    "Some exploit behaviors follow known patterns such as scanning every port in a short of time, sending a burst of request to a port... However, the biggest threat may be the one that has never been detected and classified yet. Part of detecting potential network intrusions is detecting anomalies. These are connections that aren't known to be attacks, but, do not resemble connections that have been observed in the past.\n",
    "\n",
    "In this notebook, K-means is used to detect anomalous network connections based on statistics about each of them.\n",
    "\n",
    "\n",
    "## 2.1. Data\n",
    "The data comes from [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html). The dataset is about 708MB and contains about 4.9M connections. For each connection, the data set contains information like the number of bytes sent, login attempts, TCP errors, and so on. Each connection is one line of CSV-formatted data, containing 38 features: back, buffer_overflow, ftp_write, guess_passwd, imap, ipsweep, land, loadmodule, multihop, neptune, nmap, normal, perl, phf, pod, portsweep, rootkit, satan, smurf, spy, teardrop, warezclient, warezmaster. For more details about each feature, please follow this [link](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html).\n",
    "\n",
    "Many features take on the value 0 or 1, indicating the presence or absence of a behavior such as `su_attempted` in the 15th column. Some features are counts, like `num_file_creations` in the 17th columns. Some others are the number of sent and received bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Clustering without using categorical features\n",
    "\n",
    "First, we need to import some packages that are used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as func\n",
    "import matplotlib.patches as mpatches\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from operator import add\n",
    "import seaborn as sns\n",
    "\n",
    "input_path = \"/datasets/k-means/kddcup.data\"\n",
    "raw_data = sc.textFile(input_path, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2.1. Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of features: numerical features and categorical features.\n",
    "Currently, to get familiar with the data and the problem, we only use numerical features. In our data, we also have pre-defined groups for each connection, which we can use later as our \"ground truth\" for verifying our results.\n",
    "\n",
    "**Note 1**: we don't use the labels in the training phase!!!\n",
    "\n",
    "**Note 2**: in general, since clustering is un-supervised, you don't have access to ground truth. For this reason, several metrics to judge the quality of clustering have been devised. For a short overview of such metrics, follow this [link](https://en.wikipedia.org/wiki/Cluster_analysis#Internal_evaluation). Note that computing such metrics, that is trying to assess the quality of your clustering results, is as computationally intensive as computing the clustering itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Write function `parseLine` to construct a tuple of `(label, vector)` for each connection, extract the data that contains only the data points (without label), then print the number of connections.\n",
    "\n",
    "</div>\n",
    "\n",
    "Where,\n",
    "\n",
    "* `label` is the pre-defined label of each connection\n",
    "* `vector` is a numpy array that contains values of all features, but the label and the categorial features at index `1,2,3` of each connection. Each `vector` is a data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    cols = line.split(\",\")\n",
    "    # label is the last column\n",
    "    label = cols[-1]\n",
    "    \n",
    "    # vector is every column, except the label\n",
    "    vector = cols[:-1]\n",
    "    \n",
    "    # delete values of columns that have index 1->3 (categorical features)\n",
    "    vector = [x for i, x in enumerate(vector) if i not in [1, 2, 3]]\n",
    "    \n",
    "    # convert each value from string to float\n",
    "    vector = np.array(vector, dtype=np.float)\n",
    "    \n",
    "    return (label, vector)\n",
    "\n",
    "labelsAndData = raw_data.map(parseLine).cache()\n",
    "\n",
    "# we only need the data, not the label\n",
    "data = labelsAndData.map(lambda x : x[1]).cache()\n",
    "\n",
    "# number of connections\n",
    "n = data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of connections:\", n)\n",
    "print(\"Details per connection:\", len(data.first()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = labelsAndData.map(lambda x: x[0]).distinct().count()\n",
    "print(\"Total number of labels:\\n\", x)\n",
    "y = labelsAndData.map(lambda x : (x[0], 1)).reduceByKey(add).collect()\n",
    "print(\"The labels available and their occurrence in the dataset is:\\n\")\n",
    "for _ in y:\n",
    "    print(\"%20s\\t%d\" %(_[0], _[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "TODO\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Using K-means algorithm of MLLIB, cluster the connections into two groups then plot the result. Why two groups? In this case, we are just warming up, we're testing things around, so \"two groups\" has no particular meaning.\n",
    "\n",
    "</div>  \n",
    "\n",
    "You can use the following parameters:  \n",
    "\n",
    "<ul>\n",
    "\n",
    "  <li>`maxIterations=10`</li>\n",
    "  <li>`runs=10`</li>\n",
    "  <li>`initializationMode=\"random\"`</li>\n",
    "\n",
    "</ul>  \n",
    "\n",
    "Discuss the result from your figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = KMeans.train(data, 2, maxIterations=10, runs=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Each connection has 38 attributes. If we want to cluster the data we have to decide which are the best features to use on the axis to plot the data. For this first example, let's simply use the first 3 features to do a 3D plot.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "First of all we get the values of the 3 axis and then, we scatter the plots. We will not plot all of the 4 million connections but only a subset of those.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data0x = data.filter(lambda x: clusters.predict(x)==0).map(lambda x: x[0]).take(50)\n",
    "data0y = data.filter(lambda x: clusters.predict(x)==0).map(lambda x: x[1]).take(50)\n",
    "data0z = data.filter(lambda x: clusters.predict(x)==0).map(lambda x: x[2]).take(50)\n",
    "\n",
    "data1x = data.filter(lambda x: clusters.predict(x)==1).map(lambda x: x[0]).take(50)\n",
    "data1y = data.filter(lambda x: clusters.predict(x)==1).map(lambda x: x[1]).take(50)\n",
    "data1z = data.filter(lambda x: clusters.predict(x)==1).map(lambda x: x[2]).take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(111, projection=\"3d\")\n",
    "plt.scatter(data0x, data0y, data0z, c='r')\n",
    "plt.scatter(data1x, data1y, data1z, c='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "__ = labelsAndData.map(lambda x : (clusters.predict(x[1]), 1)).reduceByKey(add).collect()\n",
    "print(__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "\n",
    "DAFAQ\n",
    "\n",
    "TODO: fix plot?\n",
    "TODO: comment last box, no predictions on label 1, all on label 0\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Evaluating model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "One of the simplest method to evaluate our result is calculate the Within Set Sum of Squared Errors (WSSSE), or simply, 'Sum of Squared Errors'. An error of a data point is defined as it's distance to the closest cluster center.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(clusters, point):\n",
    "    closest_center = clusters.centers[clusters.predict(point)]\n",
    "    return euclidean_distance(closest_center, point)**2\n",
    "t1 = time.time()\n",
    "WSSSE = data.map(lambda x : error(clusters, x)).reduce(add)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "print(\"Time needed:\", time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "print(clusters.computeCost(data))\n",
    "print(\"Time needed:\", time.time()-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "In the KMeans documentation, we found a method called <code>public double computeCost(RDD&lt;Vector&gt; data)</code> that actually computer the same information but in a much faster way.\n",
    "<br>\n",
    "We wrote the time of both methods and the improving factor is approximately 10.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Question 5\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "This is a good opportunity to use the given labels to get an intuitive sense of what went into these two clusters, by counting the labels within each cluster. Complete the following code that uses the model to assign each data point to a cluster, and counts occurrences of cluster and label pairs.  \n",
    "\n",
    "What do you think about the result?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusterLabelCount = labelsAndData.map(lambda x : ((clusters.predict(x[1]), x[0]), 1)) \\\n",
    "                        .reduceByKey(add).collect()\n",
    "\n",
    "for item in clusterLabelCount:\n",
    "    print(\"Label #%d\\tClass:%25s\\tOccurrences:%10d\" %(item[0][0],item[0][1],item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "This is a clear evidence on why the clustering didn't work well. The clusters are heavily unbalanced due to the disparity of the whole dataset. The choice of k=2 is not relevant for this comment even though, since there are 23 classes, it would make much more sense to use k=23.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4. Choosing K\n",
    "\n",
    "How many clusters are appropriate for a dataset? In particular, for our own dataset, it's clear that there are 23 distinct behavior patterns in the data, so it seems that k could be at least 23, or likely, even more. In other cases, we even don't have any information about the number of patterns at all (remember, generally your data is not labelled!). Our task now is finding a good value of $k$. For doing that, we have to build and evaluate models with different values of $k$. A clustering could be considered good if each data point were near to its closest centroid. One of the ways to evaluate a model is calculating the Mean of Squared Errors of all data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "<div class=\"alert alert-info\">\n",
    "Complete the function below to calculate the MSE of each model that is corresponding to each value of $k$.  \n",
    "\n",
    "\n",
    "Plot the results. From the obtained result, what is the best value for $k$? Why?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><h1> don't run the next cell </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# k: the number of clusters\n",
    "def clusteringScore(data, k):\n",
    "    clusters = KMeans.train(data, k, maxIterations=10, runs=10, initializationMode=\"random\")\n",
    "    # calculate mean square error\n",
    "    return clusters.computeCost(data)\n",
    "\n",
    "k_vector = [2,5,10,23,30,50,70,100,120,200,300,400]\n",
    "\n",
    "scores = [clusteringScore(data, k) for k in k_vector]\n",
    "for i, score in enumerate(scores):\n",
    "    print(\"k = %3d, WSSSE = %d\" %(k_vector[i], score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(k_vector, scores)\n",
    "plt.scatter(k_vector, scores, color=\"r\")\n",
    "plt.axis([0, np.max(k_vector)*1.05, 0.85*np.min(scores), 1.05*np.max(scores)])\n",
    "plt.xlabel(\"number of clusters k\")\n",
    "plt.ylabel(\"Clustering score\")\n",
    "plt.xticks(k_vector)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "PUT YOUR ANSWER HERE !\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Normalizing features\n",
    "\n",
    "K-means clustering treats equally all dimensions/directions of the space and therefore tends to produce more or less spherical (rather than elongated) clusters. In this situation, leaving variances uneven is equivalent to putting more weight on variables with smaller variance, so clusters will tend to be separated along variables with greater variance.\n",
    "\n",
    "In our notebook, since Euclidean distance is used, the clusters will be influenced strongly by the magnitudes of the variables, especially by outliers. Normalizing will remove this bias. \n",
    "\n",
    "Each feature can be normalized by converting it to a standard score. This means subtracting the mean of the feature’s values from each value, and dividing by the standard deviation\n",
    "\n",
    "$normalize_i=\\frac{feature_i - \\mu_i}{\\sigma_i}$\n",
    "\n",
    "Where,\n",
    "\n",
    "* $normalize_i$ is the normalized value of feature $i$\n",
    "* $\\mu_i$ is the mean of feature $i$\n",
    "* $\\sigma_i$ is the standard deviation of feature $i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Complete the code below to normalize the data. Print the first 5 lines of the new data.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"label label-success\">HINT</div> If $\\sigma_i = 0$ then $normalize_i=feature_i - \\mu_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalizeData(data):\n",
    "    # number of connections\n",
    "    n = data.count()\n",
    "\n",
    "    # calculate the sum of each feature\n",
    "    sums = data.reduce(add)\n",
    "    print(sums)\n",
    "\n",
    "    # calculate means\n",
    "    means = sums/n\n",
    "\n",
    "    # calculate the sum square of each feature\n",
    "    sumSquares = data.map(lambda x : x**2).reduce(add)\n",
    "    #print(sumSquares)\n",
    "\n",
    "    # calculate standard deviation of each feature\n",
    "    stdevs = np.sqrt(sumSquares/n - means**2)\n",
    "    stdevs[stdevs==0] = 1\n",
    "    #print(stdevs)\n",
    "\n",
    "    def normalize(point):\n",
    "        return (point-means)/stdevs\n",
    "\n",
    "    return data.map(normalize)\n",
    "\n",
    "normalizedData = normalizeData(data).cache()\n",
    "print(normalizedData.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Using the new data, build different models with different values of $k \\in [60,70,80,90,100,110]$. Evaluate the results by plotting them and choose the best value of $k$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_vector = np.arange(60, 120, 10)\n",
    "scoresNormalized = [clusteringScore(normalizedData, k) for k in k_vector]\n",
    "for i, score in enumerate(scoresNormalized):\n",
    "    print(\"k = %3d, WSSSE = %d\" %(k_vector[i], score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(k_vector, scoresNormalized)\n",
    "plt.scatter(k_vector, scoresNormalized, color=\"g\")\n",
    "plt.axis([0, np.max(k_vector)*1.05, \\\n",
    "          0.85*np.min(scoresNormalized), 1.05*np.max(scoresNormalized)])\n",
    "plt.xlabel(\"number of clusters k\")\n",
    "plt.ylabel(\"Clustering score\")\n",
    "plt.xticks(k_vector)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "\n",
    "PUT YOUR ANSWER HERE !!!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Plot the clustering result to see the difference between before and after normalizing features. Discuss about the difference and explain why and if normalization was useful.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "\n",
    "PUT YOUR ANSWER HERE !!!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Clustering using categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Loading data\n",
    "\n",
    "In the previous section, we ignored the categorical features of our data: this is not a good idea, since these categorical features can be important in providing useful information for clustering.\n",
    "The problem is that K-means (or at least, the one we have developed and the one we use from MLLib) only work with data points in a metric space. Informally, this means that operations such as addition, subtraction and computing the mean of data points are trivial and well defined. For a more formal definition of what a metric space is, follow this [link](https://en.wikipedia.org/wiki/Metric_space#Definition).\n",
    "\n",
    "What we will do next is to transform each categorical feature into one or more numerical features. This approach is very widespread: imagine for example you wanted to use K-means to cluster text data. Then, the idea is to transform text data in $d$-dimensional vectors, and a nice way to do it is to use [word2vec](http://deeplearning4j.org/word2vec). If you're interested, follow this link to a nice [blog post](http://bigdatasciencebootcamp.com/posts/Part_3/clustering_news.html) on the problem.\n",
    "\n",
    "There are two approaches:\n",
    "\n",
    "* **Approach 1**: mapping **one** categorical feature to **one** numerical feature. The values in each categorical feature are encoded into unique numbers of the new numerical feature. For example, ['VERY HOT','HOT', 'COOL', 'COLD', 'VERY COLD'] will be encoded into [0,1,2,3,4,5]. However, by using this method, we implicit assume that the value of 'VERY HOT' is smaller than 'HOT'... This is not generally true.\n",
    "\n",
    "* **Approach 2**: mapping **one** categorical feature to **multiple** numerical features. Basically, a single variable with $n$ observations and $d$ distinct values, to $d$ binary variables with $n$ observations each. Each observation indicating the presence (1) or absence (0) of the $d^{th}$ binary variable. For example, ['house', 'car', 'tooth', 'car'] becomes \n",
    "```\n",
    "[\n",
    "[1,0,0,0],\n",
    "[0,1,0,0],\n",
    "[0,0,1,0],\n",
    "[0,0,0,1],\n",
    "]\n",
    "```\n",
    "\n",
    "We call the second approach \"one-hot encoding\". By using this approach, we keep the same role for all values of categorical features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Calculate the number of distinct categorical features value (at index `1,2,3`). Then construct a new input data using one-hot encoding for these categorical features (don't throw away numerical features!).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat1 = raw_data.map(lambda x : x.split(\",\")[1]).distinct().collect()\n",
    "feat2 = raw_data.map(lambda x : x.split(\",\")[2]).distinct().collect()\n",
    "feat3 = raw_data.map(lambda x : x.split(\",\")[3]).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "TODO: let's print and tell what this data is.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parseLineWithHotEncoding(line):\n",
    "    cols = line.split(\",\")\n",
    "    # label is the last column\n",
    "    label = cols[-1]\n",
    "    \n",
    "    vector = cols[0:-1]\n",
    "    \n",
    "    # the binary features that are encoded from the first categorical feature\n",
    "    featureOfCol1 = [0]*len(feat1)\n",
    "    featureOfCol1[featureOfCol1.index(vector[1])] = 1\n",
    "    # the binary features that are encoded from the second categorical feature\n",
    "    featureOfCol2 = [0]*len(feat2)\n",
    "    featureOfCol2[featureOfCol2.index(vector[2])] = 1\n",
    "    # the binary features that are encoded from the third categorical feature\n",
    "    featureOfCol3 = [0]*len(feat3)\n",
    "    featureOfCol3[featureOfCol3.index(vector[3])] = 1\n",
    "    \n",
    "    # construct the new vector\n",
    "    vector = ([vector[0]] + featureOfCol1 + featureOfCol2 + \n",
    "        featureOfCol3 + vector[4:])\n",
    "    \n",
    "    # convert each value from string to float\n",
    "    vector = np.array(vector, dtype=np.float)\n",
    "    \n",
    "    return (label, vector)\n",
    "\n",
    "labelsAndData = raw_data.map(parseLine)\n",
    "\n",
    "# we only need the data, not the label\n",
    "data = labelsAndData.values().cache()\n",
    "\n",
    "normalizedData = normalizeData(data).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Building models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Using the new data, cluster the connections with different values of $k \\in [80,90,100,110,120,130,140,150,160]$.\n",
    "Evaluate the results and choose the best value of $k$ as previous questions.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k_vector = np.arange(80,170,10)\n",
    "scores = [clusteringScore(normalizedData, k) for k in k_vector]\n",
    "for i, score in enumerate(scores):\n",
    "    print(\"k = %3d, WSSSE = %d\" %(k_vector[i], score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(k_vector, scoresNormalized)\n",
    "plt.scatter(k_vector, scoresNormalized, color=\"g\")\n",
    "plt.axis([0, np.max(k_vector)*1.05, \\\n",
    "          0.85*np.min(scoresNormalized), 1.05*np.max(scoresNormalized)])\n",
    "plt.xlabel(\"number of clusters k\")\n",
    "plt.ylabel(\"Clustering score\")\n",
    "plt.xticks(k_vector)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "\n",
    "PUT YOUR ANSWER HERE !!!\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.4. Anomaly detection\n",
    "When we have a new connection data (e.g., one that we never saw before), we simply find the closest cluster for it, and use this information as a proxy to indicate whether the data point is anomalous or not. A simple approach to decide when there is an anomaly or not, amounts to measuring the new data point’s distance to its nearest centroid. If this distance exceeds some thresholds, it is anomalous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Build your model with the best value of $k$ in your opinion. Then, detect the anomalous connections in our data. Plot and discuss your result.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"label label-success\">HINT</div> The threshold has strong impact on the result. Be careful when choosing it! A simple way to choose the threshold's value is picking up a distance of a data point from among known data. For example, the 100th-farthest data point distance can be an option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bestK = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = KMeans.train(data, bestK, maxIterations=10, runs=10, initializationMode=\"random\")\n",
    "ordered = normalizedData.map(lambda x : (error(clusters, x), x)).takeOrdered(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "TODO: plot the line of distance from center for the points ordered by increasing distance\n",
    "<br>\n",
    "TODO: understand which is the best threshold and plot number of anomalies for given threshold\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Question 13\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Try other methods to find the best value for $k$ such as `silhouette`, `entropy`... In particular, with this data, you can take advantage of predefined labels to calculate the quality of model using entropy... However, we suggest you to try with `silhouette`. It's more general and can work with any dataset (with and without predefined labels).\n",
    "\n",
    "</div>\n",
    "\n",
    "Here are some additional information about the metrics we suggest to use:\n",
    "- <a href=\"https://en.wikipedia.org/wiki/Silhouette_(clustering)\">Silhouette</a>\n",
    "- [Hack approach to Silhouette](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n",
    "- [Entropy](http://scikit-learn.org/stable/modules/clustering.html) [Lookup for entropy]\n",
    "\n",
    "<div class=\"label label-danger\">Note</div> you are free to play with any relevant evaluation metric you think appropriate for your work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Implement K-means on Spark so that It can work with large datasets in parallel. Test your algorithm with our dataset in this notebook. Compare our algorithm with the algorithm from MLLIB.  \n",
    "\n",
    "<ul></ul>\n",
    "\n",
    "Let's clarify the meaning of this question: what we want is for students to design the K-means algorithm for the parallel programming model exposed by Spark. You are strongly invited to use the Python API (pyspark). So, at the end of the day, you will operate on RDDs, and implement a `map/reduce` algorithm that performs the two phases of the standard K-means algorithm, i.e. the assignment step and the update step.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
